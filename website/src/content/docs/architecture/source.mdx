---
title: Source Processing
description: How Blizzard reads and parses compressed NDJSON files from cloud storage
---

import PollingCaveat from '../../../components/PollingCaveat.astro';

Blizzard's source layer handles reading compressed NDJSON files from cloud storage, decompressing them, and parsing the JSON into Arrow RecordBatches for efficient columnar processing.

## Source Pipeline

```d2
direction: down
Source Processing: {
  cloud: Cloud Storage {
    label: "Cloud Storage\ns3://bucket/data/*.ndjson.gz"
    files: |
      file1.ndjson.gz
      file2.ndjson.gz
      file3.ndjson.gz
    |
  }
  download: Download (I/O)
  decompress: Decompress {
    label: "Decompress\ngzip / zstd"
  }
  parse: Parse JSON {
    label: "Parse JSON\nArrow JSON decoder"
  }
  batches: RecordBatches {
    label: "RecordBatches\nColumnar Arrow format"
  }

  cloud -> download
  download -> decompress: Bytes
  decompress -> parse: NDJSON
  parse -> batches
}
```

## File Discovery

Blizzard discovers source files by listing the configured path and filtering by extension:

```yaml
source:
  path: "s3://bucket/data/events"  # Lists recursively
```

Behavior:
- Recursively lists all files under the path
- Filters to `.ndjson.gz` files only
- Sorts files alphabetically for deterministic ordering
- Filters out already-processed files (from checkpoint)

<PollingCaveat />

## Compression Formats

| Format | Extension | Library | Description |
|--------|-----------|---------|-------------|
| **Gzip** | `.gz` | flate2 | Default, widely compatible |
| **Zstd** | `.zst` | zstd | Better compression ratio and speed |
| **None** | `.ndjson` | - | Uncompressed NDJSON |

Configuration:

```yaml
source:
  compression: gzip  # gzip (default), zstd, or none
```

## NDJSON Format

Blizzard expects newline-delimited JSON (NDJSON) where each line is a valid JSON object:

```json
{"id": "1", "timestamp": "2024-01-01T00:00:00Z", "value": 42}
{"id": "2", "timestamp": "2024-01-01T00:00:01Z", "value": 43}
{"id": "3", "timestamp": "2024-01-01T00:00:02Z", "value": 44}
```

Requirements:
- One JSON object per line
- No trailing commas
- UTF-8 encoded
- Lines can be any length

## Schema Mapping

JSON fields are mapped to Arrow/Parquet types based on the schema configuration:

| JSON Type | Config Type | Arrow Type | Notes |
|-----------|-------------|------------|-------|
| string | `string` | `Utf8` | UTF-8 string |
| number | `int32` | `Int32` | 32-bit signed integer |
| number | `int64` | `Int64` | 64-bit signed integer |
| number | `float32` | `Float32` | 32-bit float |
| number | `float64` | `Float64` | 64-bit float |
| boolean | `boolean` | `Boolean` | true/false |
| string | `timestamp` | `Timestamp(Microsecond, UTC)` | ISO 8601 format |
| string | `date` | `Date32` | Date without time |
| object/array | `json` | `Utf8` | Stored as JSON string |
| string (base64) | `binary` | `Binary` | Base64-decoded bytes |

## Schema Inference

Instead of manually specifying field types, Blizzard can automatically infer the schema from your source data:

```yaml
schema:
  infer: true
```

### How It Works

1. **Sampling**: Blizzard reads up to 1000 records from the first available NDJSON file
2. **Type Detection**: Arrow's JSON reader analyzes the sampled records to determine field types
3. **Schema Building**: A unified schema is built from the detected types

Inferred type mappings:

| JSON Value | Inferred Arrow Type |
|------------|---------------------|
| `"string"` | `Utf8` |
| `123` | `Int64` |
| `1.23` | `Float64` |
| `true`/`false` | `Boolean` |
| `null` | `Utf8` (if all null) |
| `[...]` | `List<T>` |
| `{...}` | `Struct` |

### Type Conflict Resolution

When the same field has different types across records, Arrow's schema inference fails by default. This commonly happens with:

- Fields that are sometimes `null` and sometimes a value
- Fields that alternate between objects and strings
- Fields that mix numbers and strings
- Optional nested objects that are sometimes omitted

Enable `coerce_conflicts_to_utf8` to handle these cases:

```yaml
schema:
  infer: true
  coerce_conflicts_to_utf8: true
```

#### How It Works

1. **Inference phase**: Blizzard samples records and detects type conflicts
2. **Fallback inference**: Conflicting fields are marked as `Utf8` (string)
3. **Reading phase**: Non-string values in `Utf8` fields are automatically stringified

#### Conflict Examples

**Object vs String:**
```json
{"data": {"nested": "value"}}
{"data": "just a string"}
```
Result: `data` becomes `Utf8`, objects are JSON-serialized.

**Number vs String:**
```json
{"threshold": 0.00025}
{"threshold": "disabled"}
```
Result: `threshold` becomes `Utf8`, `0.00025` → `"0.00025"`.

**Null vs Number:**
```json
{"count": null}
{"count": 42}
```
Result: Usually infers as `Int64` (nulls don't conflict). But if mixed with strings, becomes `Utf8`.

**Nested Object vs Null:**
```json
{"settings": {"timeout": 30}}
{"settings": null}
```
Result: `settings` stays as `Struct` (nulls are allowed).

#### Type Coercion During Reading

When `coerce_conflicts_to_utf8` is enabled, the reader automatically converts mismatched types:

| Actual Value | Schema Type | Result |
|--------------|-------------|--------|
| `{"a": 1}` | `Utf8` | `"{\"a\":1}"` |
| `[1, 2, 3]` | `Utf8` | `"[1,2,3]"` |
| `0.00025` | `Utf8` | `"0.00025"` |
| `true` | `Utf8` | `"true"` |
| `null` | `Utf8` | `null` |
| `"hello"` | `Utf8` | `"hello"` |

#### Metrics

When type conflicts are resolved during inference, Blizzard emits:

| Metric | Description |
|--------|-------------|
| `blizzard_schema_type_conflicts_total` | Number of fields with type conflicts coerced to Utf8 |

#### Performance Note

The coercing reader is slightly slower than the standard Arrow reader because it:
1. Parses each JSON line with serde_json
2. Walks through fields to coerce types
3. Re-serializes before passing to Arrow

For pipelines without type conflicts, leave `coerce_conflicts_to_utf8: false` (the default) to use the faster direct Arrow reader.

### When to Use Inference

| Use Case | Recommendation |
|----------|----------------|
| Prototyping / exploration | ✅ Use `infer: true` |
| Schema changes frequently | ✅ Use `infer: true` |
| Production with stable schema | ⚠️ Prefer explicit schema |
| Need specific types (e.g., `int32` vs `int64`) | ❌ Use explicit schema |
| Timestamps need parsing | ❌ Use explicit schema with `timestamp` type |

## Batching

Records are parsed into Arrow RecordBatches for efficient columnar processing:

```yaml
source:
  batch_size: 8192  # Records per batch (default: 8192)
```

The batch size affects:
- Memory usage during parsing
- Granularity of checkpoint recovery
- Write amplification in Parquet

## Checkpoint Recovery

When recovering from a checkpoint, the source layer skips already-processed records:

```d2
direction: right
checkpoint: "Checkpoint: file2.ndjson.gz → RecordsRead(5000)"

file: "file2.ndjson.gz (10000 records)" {
  skip: "[0-4999]\nSKIP" {
    style.fill: "#ffcccc"
  }
  process: "[5000-9999]\nPROCESS" {
    style.fill: "#ccffcc"
  }
}
```

The skip logic is handled in the JSON decoder:
1. Parse records into batches
2. If `skip_records > 0`, slice or skip batches accordingly
3. Only emit records after the skip point

## Concurrent Downloads

Files are downloaded concurrently to maximize I/O throughput:

```yaml
source:
  max_concurrent_files: 4  # Concurrent downloads (default: 4)
```

The downloader task:
1. Maintains a pool of concurrent download futures
2. Sends completed downloads through a bounded channel
3. Respects backpressure from the processing stage

## Error Handling

Source errors are handled gracefully:

| Error Type | Behavior |
|------------|----------|
| File not found (404) | Skip file, log warning, continue |
| Download failure | Record to DLQ, increment failure count |
| Decompression failure | Record to DLQ, increment failure count |
| Parse failure | Record to DLQ, increment failure count |

With `max_failures` configured, the pipeline stops if too many files fail:

```yaml
error_handling:
  max_failures: 10  # Stop after 10 failures (0 = unlimited)
```

## Source State

The `SourceState` tracks file processing progress:

| State | Description |
|-------|-------------|
| `Finished` | File completely processed |
| `RecordsRead(n)` | Partially processed with `n` records read |
| (not present) | Not yet processed |

```json
{
  "files": {
    "file1.ndjson.gz": "Finished",
    "file2.ndjson.gz": {"RecordsRead": 5000},
    "file3.ndjson.gz": "Finished"
  }
}
```

## Metrics

| Metric | Type | Description |
|--------|------|-------------|
| `blizzard_bytes_read_total` | Counter | Compressed bytes downloaded |
| `blizzard_file_download_duration_seconds` | Histogram | Download latency |
| `blizzard_file_decompression_duration_seconds` | Histogram | Decompression latency |
| `blizzard_active_downloads` | Gauge | Currently downloading files |
| `blizzard_decompression_queue_depth` | Gauge | Files waiting to decompress |
| `blizzard_recovered_records_total` | Counter | Records skipped during recovery |
| `blizzard_schema_type_conflicts_total` | Counter | Fields with type conflicts coerced to Utf8 |

## Configuration Example

```yaml
source:
  path: "s3://my-bucket/events/2024"
  compression: gzip
  batch_size: 8192
  max_concurrent_files: 4
  storage_options:
    AWS_REGION: "us-east-1"
```